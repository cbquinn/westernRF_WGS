
make list of unique samples
```R
library(tidyverse)
f <- read_tsv("/group/ctbrowngrp3/scratch/cbquinn/fox/fastqlist.txt", col_names=c("JOB",	"RUN",	"RFID",	"SID",	"BASENAME",	"LIB",	"PLATFORM", "POP")) 

mylist <- f %>%
	select(RFID, SID, POP) %>%
	distinct() %>%
	unite("sampleID", c(POP, SID), sep="_", remove=FALSE) %>%
	mutate(keep=case_when(POP %in% c("SVNN", "SNF1", "LAS2019") ~ FALSE,
	SID == "F12-225" ~ FALSE,
	TRUE ~ TRUE)) %>% 
	arrange(RFID)

# RF samples 1-33 should be arranged by sampleID
fround1 <- f[1:54,]

mylist <- bind_rows( mylist %>%
	filter(SID %in% unique(fround1$SID)) %>%
	arrange(sampleID) ,
	mylist %>%
		filter(!SID %in% unique(fround1$SID)) %>%
		arrange(sampleID) 
		)

write_tsv(mylist,"/group/ctbrowngrp3/scratch/cbquinn/fox/bams/samplelist.txt", col_names=FALSE)
```

List looks like this (but without headers):
```
RFID    sampleID        SID     POP     keep
RF1     SN_S11-0008     S11-0008        SN      TRUE
RF2     SNF1_S15-0575   S15-0575        SNF1    FALSE
RF3     RM_S13-3301     S13-3301        RM      TRUE
RF4     RM_S13-3312     S13-3312        RM      TRUE
RF5     LAS_M01 M01     LAS     TRUE
RF6     LAS_F01 F01     LAS     TRUE
...
```

ones that are ready: 1-18,19-20,23-33
ran 1, 5, 13, 23, 26,31
then ran 2-4,6-12,14-20,24-25,27-30,32,33
for run5, need to run 34-43
had to rerun these because fo the screwy bam swap snafu:  job arrays 13,17, 30 

/group/ctbrowngrp2/cbquinn/fox4/slurmscripts/bwa_clean.sh
```sh
#!/bin/bash -l
#SBATCH --job-name=clean
#SBATCH --array=13,17,30
#SBATCH --time 23:40:00
#SBATCH --mem=16GB
#SBATCH -p bmm
#SBATCH -A ctbrowngrp
#SBATCH -o /group/ctbrowngrp2/cbquinn/fox4/slurmlogs/clean_%a.out
#SBATCH -e /group/ctbrowngrp2/cbquinn/fox4/slurmlogs/clean_%a.err

STARTTIME=$(date +%s)

# make things fail on errors
set -o nounset
set -o errexit

echo "My SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID

LIST=/group/ctbrowngrp3/scratch/cbquinn/fox/bams/samplelist.txt
SAMPLE=$(sed "${SLURM_ARRAY_TASK_ID}q;d" $LIST | cut -f2)
TEMPDIR=/group/ctbrowngrp3/scratch/cbquinn/fox/bams/
OUTDIR=/group/ctbrowngrp2/cbquinn/fox4/0_bams/
REF=/group/ctbrowngrp2/cbquinn/fox4/ref/GCF_018345385.1/GCF_018345385.1_ASM1834538v1_genomic.fna
BED=/group/ctbrowngrp2/cbquinn/fox4/ref/GCF_018345385.1/autosomes.bed

module load samtools/1.16.1
module load picardtools/2.7.1

cd $TEMPDIR

# merge split bams

INPUTS_LIST=$( cat <(ls ${TEMPDIR}chunk/readgroups/${SAMPLE}*.bam))
INPUTS=$(echo $INPUTS_LIST | tr "\n" " ")
INPUTS_NUMB=$(echo $INPUTS_LIST | wc -l)

echo "Samtools will now merge $INPUTS_NUMB files:"
echo "samtools merge -f ${TEMPDIR}merged/${SAMPLE}.bam $INPUTS"

samtools merge -f ${TEMPDIR}merged/${SAMPLE}.bam $INPUTS

# mark duplicates

echo "marking duplicates..."
mkdir -p deduped
java -jar ${PICARD}/picard.jar MarkDuplicates \
    VALIDATION_STRINGENCY=LENIENT \
      I=${TEMPDIR}merged/${SAMPLE}.bam \
      O=${TEMPDIR}deduped/${SAMPLE}.bam \
      M=${TEMPDIR}deduped/qcmetrics/${SAMPLE}_duplicationMETRICS.txt

# indel realignment

echo "indexing..."
samtools index ${TEMPDIR}deduped/${SAMPLE}.bam

module load GATK/3.6

java -Xmx4g -jar ${GATK_HOME}/GenomeAnalysisTK.jar \
-T RealignerTargetCreator \
-nt 4 \
-R ${REF} \
-I ${TEMPDIR}deduped/${SAMPLE}.bam \
-o ${TEMPDIR}deduped/intervals/${SAMPLE}.intervals \
-log ${TEMPDIR}deduped/intervals/${SAMPLE}.intervals.log

java -Xmx4g -jar ${GATK_HOME}/GenomeAnalysisTK.jar \
-T IndelRealigner \
-R ${REF} \
-I ${TEMPDIR}deduped/${SAMPLE}.bam \
-targetIntervals ${TEMPDIR}deduped/intervals/${SAMPLE}.intervals \
-o ${OUTDIR}indelrealigned/${SAMPLE}.bam

echo "indexing..."
samtools index ${OUTDIR}indelrealigned/${SAMPLE}.bam

echo "getting stats from GATK..."
# collect allignment metrics
java -jar ${PICARD}/picard.jar CollectAlignmentSummaryMetrics \
          R=${REF} \
          I=${OUTDIR}indelrealigned/${SAMPLE}.bam \
          O=${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_summaryMETRICS.txt \
          VALIDATION_STRINGENCY=LENIENT

echo "getting stats from samtools..."

# count total reads
samtools view -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_tot.flagstat

# count number of good reads: exclude umapped reads, secondary and chimeric reads, singleton reads, and clones
samtools view -F 1796 -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_1796.flagstat

# count number of good reads with mapq > 30: exclude umapped reads, secondary and chimeric reads, singleton reads, and clones
samtools view -F 1796 -q 30 -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_1796q30.flagstat

# count number of reads that map to more than one location and/or have an umapped mate
samtools view -f 2312 -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_2312.flagstat

# count number of unmapped reads
samtools view -f 4 -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_4.flagstat

# echo "getting  histogram of mapping quality..."
# for "good" reads (co1 is mapQ, col2 is no. reads)
samtools view -F 1796 -L $BED ${OUTDIR}indelrealigned/${SAMPLE}.bam | cut -f 5 | sort | uniq  -c | sort -n | awk '{printf("%s\t%d\n",$2,$1);}' > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_mapq.dist

echo "getting mean depth of coverage..."
# (default is to exclude reads with flag 1796)
conda activate mosdepth
mosdepth -n --fast-mode --by 50000 ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE} ${OUTDIR}indelrealigned/${SAMPLE}.bam
mosdepth -n --fast-mode --by 50000 --mapq 30 ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_mapq30 ${OUTDIR}indelrealigned/${SAMPLE}.bam

ENDTIME=$(date +%s)
TIMESPEND=$(($ENDTIME - $STARTTIME))
((sec=TIMESPEND%60,TIMESPEND/=60, min=TIMESPEND%60, hrs=TIMESPEND/60))
timestamp=$(printf "%d:%02d:%02d" $hrs $min $sec)
echo "All done. Took $timestamp hours:minutes:seconds to complete..."
```

/group/ctbrowngrp2/cbquinn/fox4/slurmscripts/bwa_clean_postfarmupdate.sh
```sh
#!/bin/bash -l
#SBATCH --job-name=clean
#SBATCH --array=13,17,30
#SBATCH --time 2-00:00:00
#SBATCH --mem=16GB
#SBATCH -p bmm
#SBATCH -A ctbrowngrp
#SBATCH -o /group/ctbrowngrp2/cbquinn/fox4/slurmlogs/clean_%a.out
#SBATCH -e /group/ctbrowngrp2/cbquinn/fox4/slurmlogs/clean_%a.err

STARTTIME=$(date +%s)

# make things fail on errors
set -o nounset
set -o errexit

echo "My SLURM_ARRAY_TASK_ID: " $SLURM_ARRAY_TASK_ID

LIST=/group/ctbrowngrp3/scratch/cbquinn/fox/bams/samplelist.txt
SAMPLE=$(sed "${SLURM_ARRAY_TASK_ID}q;d" $LIST | cut -f2)
TEMPDIR=/group/ctbrowngrp3/scratch/cbquinn/fox/bams/
OUTDIR=/group/ctbrowngrp2/cbquinn/fox4/0_bams/
REF=/group/ctbrowngrp2/cbquinn/fox4/ref/GCF_018345385.1/GCF_018345385.1_ASM1834538v1_genomic.fna
BED=/group/ctbrowngrp2/cbquinn/fox4/ref/GCF_018345385.1/autosomes.bed

#module load samtools/1.16.1 # for most samples before farm reboot
module load samtools/1.14 # for 13,17,30 after farm reboot
module load picard
module unload jdk/17.0.1
module load deprecated/jdk/1.8.0.121

cd $TEMPDIR

# merge split bams

INPUTS_LIST=$( cat <(ls ${TEMPDIR}chunk/readgroups/${SAMPLE}*.bam))
INPUTS=$(echo $INPUTS_LIST | tr "\n" " ")
INPUTS_NUMB=$(echo $INPUTS_LIST | wc -l)

echo "Samtools will now merge $INPUTS_NUMB files:"
echo "samtools merge -f ${TEMPDIR}merged/${SAMPLE}.bam $INPUTS"

samtools merge -f ${TEMPDIR}merged/${SAMPLE}.bam $INPUTS

# mark duplicates

echo "marking duplicates..."
mkdir -p deduped
picard MarkDuplicates \
    VALIDATION_STRINGENCY=LENIENT \
      I=${TEMPDIR}merged/${SAMPLE}.bam \
      O=${TEMPDIR}deduped/${SAMPLE}.bam \
      M=${TEMPDIR}deduped/qcmetrics/${SAMPLE}_duplicationMETRICS.txt

# add read groups again
RFID=$(sed "${SLURM_ARRAY_TASK_ID}q;d" $LIST | cut -f1)
SID=$(sed "${SLURM_ARRAY_TASK_ID}q;d" $LIST | cut -f3)
POP=$(sed "${SLURM_ARRAY_TASK_ID}q;d" $LIST | cut -f4)

echo "adding readgroups... again"
picard AddOrReplaceReadGroups \
    I=${TEMPDIR}deduped/${SAMPLE}.bam  \
    O=${TEMPDIR}readgroups/${SAMPLE}.bam \
    RGID=${RFID}_${SID} RGLB=merged RGPL=illumina RGPU=NULL RGSM=${RFID}_${SID} \
    VALIDATION_STRINGENCY=LENIENT

# indel realignment
module unload deprecated/jdk/1.8.0.121
module unload picard
module load gatk/3.8.1
module load deprecated/jdk/1.8.0.121

echo "indexing bam"
samtools index ${TEMPDIR}readgroups/${SAMPLE}.bam

echo "realigning indels..."

java -Xmx4g -jar ${GATK} \
-T RealignerTargetCreator \
-nt 4 \
-R ${REF} \
-I ${TEMPDIR}readgroups/${SAMPLE}.bam \
-o ${TEMPDIR}deduped/intervals/${SAMPLE}.intervals \
-log ${TEMPDIR}deduped/intervals/${SAMPLE}.intervals.log

java -Xmx4g -jar ${GATK} \
-T IndelRealigner \
-R ${REF} \
-I ${TEMPDIR}readgroups/${SAMPLE}.bam \
-targetIntervals ${TEMPDIR}deduped/intervals/${SAMPLE}.intervals \
-o ${OUTDIR}indelrealigned/${SAMPLE}.bam

echo "indexing..."
samtools index ${OUTDIR}indelrealigned/${SAMPLE}.bam

module unload gatk/3.8.1
module load picard

echo "getting stats from GATK..."
# collect allignment metrics
picard CollectAlignmentSummaryMetrics \
          R=${REF} \
          I=${OUTDIR}indelrealigned/${SAMPLE}.bam \
          O=${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_summaryMETRICS.txt \
          VALIDATION_STRINGENCY=LENIENT

echo "getting stats from samtools..."

# count total reads
samtools view -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_tot.flagstat

# count number of good reads: exclude umapped reads, secondary and chimeric reads, singleton reads, and clones
samtools view -F 1796 -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_1796.flagstat

# count number of good reads with mapq > 30: exclude umapped reads, secondary and chimeric reads, singleton reads, and clones
samtools view -F 1796 -q 30 -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_1796q30.flagstat

# count number of reads that map to more than one location and/or have an umapped mate
samtools view -f 2312 -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_2312.flagstat

# count number of unmapped reads
samtools view -f 4 -L $BED -c ${OUTDIR}indelrealigned/${SAMPLE}.bam > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_NoReads_4.flagstat

# echo "getting  histogram of mapping quality..."
# for "good" reads (co1 is mapQ, col2 is no. reads)
samtools view -F 1796 -L $BED ${OUTDIR}indelrealigned/${SAMPLE}.bam | cut -f 5 | sort | uniq  -c | sort -n | awk '{printf("%s\t%d\n",$2,$1);}' > ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_mapq.dist

echo "getting mean depth of coverage..."
# (default is to exclude reads with flag 1796)
conda activate mosdepth
mosdepth -n --fast-mode --by 50000 ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE} ${OUTDIR}indelrealigned/${SAMPLE}.bam
mosdepth -n --fast-mode --by 50000 --mapq 30 ${OUTDIR}indelrealigned/qcmetrics/${SAMPLE}_mapq30 ${OUTDIR}indelrealigned/${SAMPLE}.bam

ENDTIME=$(date +%s)
TIMESPEND=$(($ENDTIME - $STARTTIME))
((sec=TIMESPEND%60,TIMESPEND/=60, min=TIMESPEND%60, hrs=TIMESPEND/60))
timestamp=$(printf "%d:%02d:%02d" $hrs $min $sec)
echo "All done. Took $timestamp hours:minutes:seconds to complete..."
```


### get depth and read stats

```bash
cd /group/ctbrowngrp2/cbquinn/fox4/0_bams/indelrealigned/qcmetrics

# for depth summary
myfiles=$(ls *.summary*)
rm temp*.txt
for i in $myfiles
do
echo $i >> temp1.txt
echo $i >> temp2.txt
grep -w total $i | cut -f 4 >> temp1.txt
grep -w NC_054848.1 $i | cut -f 4 >> temp2.txt

done

xargs -n2 < temp1.txt > depthsummary_total.txt
xargs -n2 < temp2.txt > depthsummary_X.txt

paste -d " " <(xargs -n4 < depthsummary_total.txt) <(xargs -n4 < depthsummary_X.txt) | cut -f1,2,4,6,8 -d " " > depthsummary_allfoxes.txt

echo -e "filename q30_dp dp q30_Xdp Xdp" | cat - depthsummary_allfoxes.txt > depthsummary_allfoxes_wheader.txt


# for reads  summary
LIST=/group/ctbrowngrp3/scratch/cbquinn/fox/bams/samplelist.txt
myfiles=$(awk 'BEGIN { ORS = " " } { print $2 }' $LIST)
rm temp*.txt
for i in $myfiles
do
echo $i >> temp1.txt
cat ${i}*flagstat >> temp1.txt
done

xargs -n6 < temp1.txt > readssummary.txt
echo -e "sampleID reads1796 reads1796_q30 reads2312 reads4 reads_total" | cat - readssummary.txt > readssummary_wheader.txt


# get clonality summary
cd /group/ctbrowngrp3/scratch/cbquinn/fox/bams/deduped/qcmetrics
LIST=/group/ctbrowngrp3/scratch/cbquinn/fox/bams/samplelist.txt
myfiles=$(awk 'BEGIN { ORS = " " } { print $2 }' $LIST)
rm temp*.txt
for i in $myfiles
do
grep "^[^#;]" ${i}_duplicationMETRICS.txt | awk -F '\t' 'NF == 10' | awk -v a=$i '{print a, $1,$2,$3,$4,$5,$6,$7,$8,$9, $10}' | tail -n +2 >> temp1.txt
done

echo -e "sampleID LIBRARY UNPAIRED_READS_EXAMINED READ_PAIRS_EXAMINED SECONDARY_OR_SUPPLEMENTARY_RDS UNMAPPED_READS UNPAIRED_READ_DUPLICATES READ_PAIR_DUPLICATES READ_PAIR_OPTICAL_DUPLICATES PERCENT_DUPLICATION ESTIMATED_LIBRARY_SIZE" | cat - temp1.txt > /group/ctbrowngrp2/cbquinn/fox4/0_bams/indelrealigned/qcmetrics/duplicationsummary_wheader.txt
```


## Make mqpQ distribution plots for each individual
(use this to decide mapq=20 vs mapq=30)
```R
R
library(tidyverse)

setwd("/group/ctbrowngrp2/cbquinn/fox4/0_bams/indelrealigned/qcmetrics")

info <- read_tsv("/group/ctbrowngrp3/scratch/cbquinn/fox/bams/samplelist.txt", col_names=c("RFID", "sampleID", "SID", "pop", "keep")) %>%
select(sampleID, pop) %>%
mutate(pop=factor(pop, levels= c("SN","SNF1","LAS", "LAS2019", "ORC","WAC" , "RM",  "SV", "SVNN", "ECAN","VT", "AK","RUS"))) %>%
arrange(pop, sampleID)


myfiles <- list.files(pattern="mapq.dist$")
mylist <- lapply(myfiles, read_tsv, col_names=c("mapq", "reads"), id="sampleID")
d <- bind_rows(mylist) %>%
	mutate(sampleID=gsub("_mapq.dist", "", sampleID)) %>%
	left_join(info) %>%
	mutate(sampleID=factor(sampleID, levels=unique(info$sampleID)))

# raw counts
ggplot(d, aes(x=mapq, y=reads, fill=pop)) +
	geom_bar(stat="identity") +
	theme_bw() +
	facet_wrap(~sampleID, ncol=8, scales="free_y") +
	ggtitle("Mapping quality distributions")
ggsave("mapq_distributions.png", height=10, width=16)


# but more depth is more reads, so translate to % and show on the same scale
d.perc <- d %>%
	group_by(sampleID) %>%
	summarize(total=sum(reads)) %>%
	right_join(d) %>%
	mutate(perc=reads/total) %>%
	arrange(sampleID, desc(mapq)) 
	
d.perc.sum <- d.perc%>%
	group_by(sampleID) %>%
	summarize(pop=pop, mapq=mapq, perc=perc, perc_sum=cumsum(perc))

d.perc.summary <- d.perc.sum %>%
	filter(mapq %in% c(20,30)) %>%
	select(-perc) %>%
	pivot_wider(names_from=mapq, values_from=perc_sum) %>%
	rename(mapq_30=`30`, mapq_20=`20`) %>%
	mutate(loss=mapq_20-mapq_30)
write_tsv(d.perc.summary, "mapq_distribution_q20vsq30.tsv")

ggplot(d.perc, aes(x=mapq, y=perc, fill=pop)) +
	geom_bar(stat="identity") +
	theme_bw() +
	facet_wrap(~sampleID, ncol=8) +
	ggtitle("Mapping quality - cumulative sum")
ggsave("mapq_distributions_perc_cumsum.png", height=10, width=16)

ggplot(d.perc, aes(x=mapq, y=perc, fill=pop)) +
	geom_bar(stat="identity") +
	theme_bw() +
	facet_wrap(~sampleID, ncol=8) +
	ggtitle("Mapping quality - cumulative sum") +
	coord_cartesian(ylim=c(0,.01))
ggsave("mapq_distributions_perc_cumsum_zoomed.png", height=10, width=16)

```

Mapping quality distributions are saved here: ```

```
V:\3730Data\377STRs\Wildlife\Cate\WesternRedFoxWGS\round4\QCstats
```

This table shows the % of reads kept with a mapq filter of 20 vs 30 . Even Though some of the Lassen samples are worse, the loss is negligible.  (More of the differences is in the unmapped?)

| sampleID | pop | mapq_30 | mapq_20 | loss |
| --- | --- | --- | --- | --- |
| SN_S11-0008 | SN | 0.947380708 | 0.951981562 | 0.004600854 |
| SNF1_S15-0575 | SNF1 | 0.950954608 | 0.955407377 | 0.00445277 |
| LAS_F01 | LAS | 0.941427243 | 0.946742458 | 0.005315215 |
| LAS_F02 | LAS | 0.92072632 | 0.927802343 | 0.007076023 |
| LAS_F03 | LAS | 0.939078873 | 0.944727781 | 0.005648908 |
| LAS_F05 | LAS | 0.940201689 | 0.945802119 | 0.005600431 |
| LAS_M01 | LAS | 0.934736484 | 0.94106712 | 0.006330636 |
| LAS2019_S18-1537 | LAS2019 | 0.950115998 | 0.95435019 | 0.004234191 |
| LAS2019_S18-1538 | LAS2019 | 0.951753968 | 0.95593636 | 0.004182392 |
| LAS2019_S19-0101 | LAS2019 | 0.950497522 | 0.954698283 | 0.004200761 |
| LAS2019_S19-6657 | LAS2019 | 0.950593789 | 0.954869254 | 0.004275465 |
| LAS2019_S20-1497 | LAS2019 | 0.946460468 | 0.95131656 | 0.004856092 |
| LAS2019_S20-1499 | LAS2019 | 0.946761369 | 0.951310977 | 0.004549608 |
| ORC_S13-2559 | ORC | 0.947642304 | 0.951825832 | 0.004183527 |
| ORC_S17-2543 | ORC | 0.946985037 | 0.951590928 | 0.004605891 |
| ORC_S17-2544 | ORC | 0.945991103 | 0.950620006 | 0.004628903 |
| ORC_S18-1173 | ORC | 0.948295212 | 0.952655364 | 0.004360152 |
| ORC_S18-2062 | ORC | 0.950035907 | 0.954271678 | 0.004235771 |
| ORC_S18-2068 | ORC | 0.946404256 | 0.950913309 | 0.004509053 |
| ORC_S18-2069 | ORC | 0.947246283 | 0.951815505 | 0.004569222 |
| ORC_S18-2071 | ORC | 0.950522331 | 0.954780102 | 0.00425777 |
| ORC_S18-2072 | ORC | 0.950199761 | 0.954499505 | 0.004299743 |
| WAC_S10-0511 | WAC | 0.948497954 | 0.952838625 | 0.00434067 |
| WAC_S10-0583 | WAC | 0.946781414 | 0.951318594 | 0.00453718 |
| WAC_S11-0715 | WAC | 0.950147348 | 0.954423902 | 0.004276554 |
| WAC_S11-0716 | WAC | 0.95115481 | 0.955379515 | 0.004224704 |
| WAC_S20-1700 | WAC | 0.948975238 | 0.953370685 | 0.004395447 |
| RM_S13-0649 | RM | 0.949901032 | 0.954152497 | 0.004251464 |
| RM_S13-1169 | RM | 0.950237006 | 0.954264601 | 0.004027595 |
| RM_S13-2269 | RM | 0.934579738 | 0.939935024 | 0.005355286 |
| RM_S13-3301 | RM | 0.951866009 | 0.956295142 | 0.004429133 |
| RM_S13-3312 | RM | 0.951446413 | 0.955979374 | 0.004532961 |
| RM_S18-2063 | RM | 0.948772288 | 0.953122061 | 0.004349773 |
| SVNN_S18-2045 | SVNN | 0.94632001 | 0.951087427 | 0.004767417 |
| ECAN_S14-0407 | ECAN | 0.941808571 | 0.946444689 | 0.004636119 |
| ECAN_S14-0430 | ECAN | 0.951956618 | 0.956039891 | 0.004083273 |
| VT_F12-225 | VT | 0.946265057 | 0.950430322 | 0.004165265 |

